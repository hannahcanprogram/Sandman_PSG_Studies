{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42d0bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8afc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 done, saved to group1.xlsx\n",
      "Group 2 done, saved to group2.xlsx\n",
      "Group 3 done, saved to group3.xlsx\n",
      "Group 4 done, saved to group4.xlsx\n"
     ]
    }
   ],
   "source": [
    "filefolder = r\"D:\\SandmanGitCodes\\files\"  # metadate files saved in local from prior unzip process\n",
    "out_dir = r\"D:\\SandmanGitCodes\\files\\merged\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "n = 16\n",
    "lst_files = []\n",
    "for i in range(n):\n",
    "    filename = f\"Sandman_Zipped_Metadata_Processing_part{i+1:02d}.xlsx\"\n",
    "    filepath = os.path.join(filefolder,filename)\n",
    "    lst_files.append(filepath)\n",
    "\n",
    "# groups in 4 external hard drives, each drive contain multiple partitions from unzip jobs\n",
    "group_strs = [\n",
    "    \"7&9&14&16\",\n",
    "    \"2&10&11&12&13\",\n",
    "    \"1&3&8&15\",\n",
    "    \"4&5&6\",\n",
    "]\n",
    "\n",
    "groups_files = [\n",
    "    [lst_files[int(x.strip()) - 1] for x in s.split(\"&\")]\n",
    "    for s in group_strs\n",
    "]\n",
    "\n",
    "for gi, files in enumerate(groups_files, 1):\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = pd.read_excel(f, engine=\"openpyxl\")\n",
    "        df[\"SourceFile\"] = os.path.basename(f)\n",
    "        dfs.append(df)\n",
    "    merged = pd.concat(dfs, ignore_index=True)\n",
    "    merged.to_excel(os.path.join(out_dir, f\"group{gi}.xlsx\"), index=False)\n",
    "    print(f\"Group {gi} done, saved to group{gi}.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aadc9875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\SandmanGitCodes\\files\\merged\\group1.xlsx ...\n",
      "Cleaned file saved to D:\\SandmanGitCodes\\files\\cleaned\\group1.xlsx\n",
      "Processing D:\\SandmanGitCodes\\files\\merged\\group2.xlsx ...\n",
      "Cleaned file saved to D:\\SandmanGitCodes\\files\\cleaned\\group2.xlsx\n",
      "Processing D:\\SandmanGitCodes\\files\\merged\\group3.xlsx ...\n",
      "Cleaned file saved to D:\\SandmanGitCodes\\files\\cleaned\\group3.xlsx\n",
      "Processing D:\\SandmanGitCodes\\files\\merged\\group4.xlsx ...\n",
      "Cleaned file saved to D:\\SandmanGitCodes\\files\\cleaned\\group4.xlsx\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "in_dir = r\"D:\\SandmanGitCodes\\files\\merged\"\n",
    "out_dir = r\"D:\\SandmanGitCodes\\files\\cleaned\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# loop over 4 source hard drives\n",
    "for fname in os.listdir(in_dir):\n",
    "    if fname.lower().endswith(\".xlsx\") and fname.startswith(\"group\"):\n",
    "        in_path = os.path.join(in_dir, fname)\n",
    "        print(f\"Processing {in_path} ...\")\n",
    "\n",
    "        # read\n",
    "        df = pd.read_excel(in_path, engine=\"openpyxl\")\n",
    "\n",
    "        # ============ cleansing ============\n",
    "        df[\"CleanedFileName\"] = (\n",
    "            df[\"FileName\"]\n",
    "            .str.replace(r\"[()]\", \"\", regex=True)\n",
    "            .str.replace(r\"\\.zip$\", \"\", regex=True)\n",
    "            .str.replace(r\"^_+\", \"\", regex=True)\n",
    "            .str.replace(\"'\", \"\", regex=False)\n",
    "            )\n",
    "\n",
    "        # save\n",
    "        out_path = os.path.join(out_dir, fname)\n",
    "        df.to_excel(out_path, index=False)\n",
    "        print(f\"Cleaned file saved to {out_path}\")\n",
    "\n",
    "print(\"All files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0d4ff20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\SandmanGitCodes\\files\\cleaned\\group1.xlsx ...\n",
      "group1.xlsx: 24 duplicate rows removed\n",
      "Saved subset to D:\\SandmanGitCodes\\files\\final\\group1.xlsx with 3396 rows.\n",
      "Processing D:\\SandmanGitCodes\\files\\cleaned\\group2.xlsx ...\n",
      "group2.xlsx: 25 duplicate rows removed\n",
      "Saved subset to D:\\SandmanGitCodes\\files\\final\\group2.xlsx with 4967 rows.\n",
      "Processing D:\\SandmanGitCodes\\files\\cleaned\\group3.xlsx ...\n",
      "group3.xlsx: 409 duplicate rows removed\n",
      "Saved subset to D:\\SandmanGitCodes\\files\\final\\group3.xlsx with 3577 rows.\n",
      "Processing D:\\SandmanGitCodes\\files\\cleaned\\group4.xlsx ...\n",
      "group4.xlsx: 1 duplicate rows removed\n",
      "Saved subset to D:\\SandmanGitCodes\\files\\final\\group4.xlsx with 2974 rows.\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "in_dir = r\"D:\\SandmanGitCodes\\files\\cleaned\"\n",
    "out_dir = r\"D:\\SandmanGitCodes\\files\\final\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "group_files = [f\"group{i}.xlsx\" for i in range(1, 5)]\n",
    "\n",
    "for fname in group_files:\n",
    "    in_path = os.path.join(in_dir, fname)\n",
    "    print(f\"Processing {in_path} ...\")\n",
    "\n",
    "    df = pd.read_excel(in_path)\n",
    "\n",
    "    df = df[df[\"OutputPath\"] != \"-1\"]\n",
    "\n",
    "    cols_to_keep = [\"FullPath\", \"OutputPath\", \"CleanedFileName\"]\n",
    "    subset = df[cols_to_keep].copy()\n",
    "\n",
    "    dedup = subset.drop_duplicates(subset=\"CleanedFileName\", keep=\"first\")\n",
    "\n",
    "    print(f\"{fname}: {len(subset) - len(dedup)} duplicate rows removed\")\n",
    "\n",
    "    out_path = os.path.join(out_dir, fname)\n",
    "    dedup.to_excel(out_path, index=False)\n",
    "\n",
    "    print(f\"Saved subset to {out_path} with {len(dedup)} rows.\")\n",
    "\n",
    "print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f919efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading D:\\SandmanGitCodes\\files\\final\\group1.xlsx ...\n",
      "Reading D:\\SandmanGitCodes\\files\\final\\group2.xlsx ...\n",
      "Reading D:\\SandmanGitCodes\\files\\final\\group3.xlsx ...\n",
      "Reading D:\\SandmanGitCodes\\files\\final\\group4.xlsx ...\n",
      "Merged dataframe shape: (14914, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gooda\\AppData\\Local\\Temp\\ipykernel_28096\\860522918.py:122: FutureWarning: Operation between non boolean Series with different indexes will no longer return a boolean result in a future version. Cast both Series to object type to maintain the prior behavior.\n",
      "  dup_name_id_idx = merged_df.index[mask_name_any & (name_id_sizes > 1)]\n",
      "C:\\Users\\gooda\\AppData\\Local\\Temp\\ipykernel_28096\\860522918.py:144: FutureWarning: Operation between non boolean Series with different indexes will no longer return a boolean result in a future version. Cast both Series to object type to maintain the prior behavior.\n",
      "  dup_id_idx = merged_df.index[no_name & (id_sizes_no_name > 1)]\n"
     ]
    }
   ],
   "source": [
    "in_dir = r\"D:\\SandmanGitCodes\\files\\final\"\n",
    "\n",
    "group_files = [f\"group{i}.xlsx\" for i in range(1, 5)]\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for fname in group_files:\n",
    "    in_path = os.path.join(in_dir, fname)\n",
    "    print(f\"Reading {in_path} ...\")\n",
    "\n",
    "    df = pd.read_excel(in_path)\n",
    "\n",
    "    df[\"SourceGroup\"] = fname  # \"group1.xlsx\"\n",
    "\n",
    "    all_dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "print(\"Merged dataframe shape:\", merged_df.shape)\n",
    "\n",
    "# ========== 0) ==========\n",
    "need_cols = [\"SourceGroup\", \"CleanedFileName\"]\n",
    "miss = [c for c in need_cols if c not in merged_df.columns]\n",
    "if miss:\n",
    "    raise ValueError(f\"miss: {miss}\")\n",
    "\n",
    "merged_df = merged_df.sort_values(\"CleanedFileName\", ascending=True, kind=\"mergesort\").reset_index(drop=True)\n",
    "merged_df[\"IsDup\"] = merged_df[\"CleanedFileName\"].duplicated(keep=\"first\").astype(int)\n",
    "\n",
    "DATE_TOKEN = re.compile(r'^\\d{4}-\\d{1,2}-\\d{1,2}$')\n",
    "\n",
    "def is_date_token(tok: str) -> bool:\n",
    "    return bool(DATE_TOKEN.fullmatch(tok))\n",
    "\n",
    "def extract_id_from_clean(name: str):\n",
    "    if pd.isna(name):\n",
    "        return pd.NA\n",
    "    s = str(name)\n",
    "    if s.startswith(\"_0_0_0\"):\n",
    "        return pd.NA\n",
    "    parts = s.split(\"_\")\n",
    "    start = 1 if parts and is_date_token(parts[0]) else 0\n",
    "    for tok in parts[start:]:\n",
    "        if tok.isdigit():\n",
    "            return tok\n",
    "    return pd.NA\n",
    "\n",
    "def extract_subject_from_clean(name: str):\n",
    "    if pd.isna(name):\n",
    "        return pd.NA\n",
    "    s = str(name)\n",
    "    if s.startswith(\"_0_0_0\"):\n",
    "        return pd.NA\n",
    "    parts = s.split(\"_\")\n",
    "    if parts and is_date_token(parts[0]):\n",
    "        return pd.NA\n",
    "    for i, tok in enumerate(parts):\n",
    "        if tok.isdigit():\n",
    "            subj = \"_\".join(parts[:i]).strip(\"_\")\n",
    "            return subj if subj else pd.NA\n",
    "    return pd.NA\n",
    "\n",
    "#  Subject / ID / ID_num\n",
    "if \"Subject\" not in merged_df.columns:\n",
    "    merged_df[\"Subject\"] = merged_df[\"CleanedFileName\"].apply(extract_subject_from_clean)\n",
    "if \"ID\" not in merged_df.columns:\n",
    "    merged_df[\"ID\"] = merged_df[\"CleanedFileName\"].apply(extract_id_from_clean)\n",
    "if \"ID_num\" not in merged_df.columns:\n",
    "    merged_df[\"ID_num\"] = pd.to_numeric(merged_df[\"ID\"], errors=\"coerce\")\n",
    "\n",
    "merged_df[\"Subject_norm\"] = (\n",
    "    merged_df[\"Subject\"]\n",
    "      .astype(\"string\")  # None/NA\n",
    "      .str.lower()\n",
    "      .str.strip()\n",
    ")\n",
    "\n",
    "# 0->A, 1->B, ...\n",
    "def idx_to_letters(n: int) -> str:\n",
    "    n = int(n)\n",
    "    s = \"\"\n",
    "    while True:\n",
    "        s = chr(ord('A') + (n % 26)) + s\n",
    "        n = n // 26 - 1\n",
    "        if n < 0:\n",
    "            return s\n",
    "\n",
    "# ========== 1) group by name, then by ID ==========\n",
    "has_name = merged_df[\"Subject_norm\"].notna() & (merged_df[\"Subject_norm\"] != \"\")\n",
    "\n",
    "# 1.1\n",
    "name_sizes = (\n",
    "    merged_df.loc[has_name]\n",
    "             .groupby([\"SourceGroup\", \"Subject_norm\"], sort=False)[\"Subject_norm\"]\n",
    "             .transform(\"size\")\n",
    ")\n",
    "merged_df[\"isNameDup\"] = 0\n",
    "merged_df.loc[has_name, \"isNameDup\"] = (name_sizes > 1).astype(int)\n",
    "\n",
    "# 1.2\n",
    "merged_df[\"NameGroup\"] = \"A\"\n",
    "to_rank_name_idx = merged_df.index[has_name & (merged_df[\"isNameDup\"] == 1)]\n",
    "if len(to_rank_name_idx):\n",
    "    rank_name = (\n",
    "        merged_df.loc[to_rank_name_idx]\n",
    "                 .sort_values([\"SourceGroup\", \"Subject_norm\", \"CleanedFileName\"])\n",
    "                 .groupby([\"SourceGroup\", \"Subject_norm\"], sort=False)\n",
    "                 .cumcount()\n",
    "                 .map(idx_to_letters)\n",
    "    )\n",
    "    merged_df.loc[rank_name.index, \"NameGroup\"] = rank_name\n",
    "\n",
    "# 1.3 \n",
    "merged_df[\"isIDDup_Name\"] = 0\n",
    "merged_df[\"NameIDGroup\"] = \"A\"\n",
    "mask_name_any = has_name\n",
    "name_id_sizes = (\n",
    "    merged_df.loc[mask_name_any]\n",
    "             .groupby([\"SourceGroup\", \"Subject_norm\", \"ID_num\"], sort=False)[\"ID_num\"]\n",
    "             .transform(\"size\")\n",
    ")\n",
    "dup_name_id_idx = merged_df.index[mask_name_any & (name_id_sizes > 1)]\n",
    "merged_df.loc[dup_name_id_idx, \"isIDDup_Name\"] = 1\n",
    "if len(dup_name_id_idx):\n",
    "    rank_name_id = (\n",
    "        merged_df.loc[dup_name_id_idx]\n",
    "                 .sort_values([\"SourceGroup\", \"Subject_norm\", \"ID_num\", \"CleanedFileName\"])\n",
    "                 .groupby([\"SourceGroup\", \"Subject_norm\", \"ID_num\"], sort=False)\n",
    "                 .cumcount()\n",
    "                 .map(idx_to_letters)\n",
    "    )\n",
    "    merged_df.loc[rank_name_id.index, \"NameIDGroup\"] = rank_name_id\n",
    "\n",
    "# ========== 2) name na: by ID ==========\n",
    "no_name = ~has_name & merged_df[\"ID_num\"].notna()\n",
    "merged_df[\"isIDDup_NoName\"] = 0\n",
    "merged_df[\"IDOnlyGroup\"] = \"A\"\n",
    "\n",
    "id_sizes_no_name = (\n",
    "    merged_df.loc[no_name]\n",
    "             .groupby([\"SourceGroup\", \"ID_num\"], sort=False)[\"ID_num\"]\n",
    "             .transform(\"size\")\n",
    ")\n",
    "dup_id_idx = merged_df.index[no_name & (id_sizes_no_name > 1)]\n",
    "merged_df.loc[dup_id_idx, \"isIDDup_NoName\"] = 1\n",
    "if len(dup_id_idx):\n",
    "    rank_id_only = (\n",
    "        merged_df.loc[dup_id_idx]\n",
    "                 .sort_values([\"SourceGroup\", \"ID_num\", \"CleanedFileName\"])\n",
    "                 .groupby([\"SourceGroup\", \"ID_num\"], sort=False)\n",
    "                 .cumcount()\n",
    "                 .map(idx_to_letters)\n",
    "    )\n",
    "    merged_df.loc[rank_id_only.index, \"IDOnlyGroup\"] = rank_id_only\n",
    "\n",
    "# ========== 3) FinalGroup ==========\n",
    "# 规则：\n",
    "# - duplicated names → FinalGroup = NameGroup\n",
    "# - duplicated ids→ FinalGroup = IDOnlyGroup\n",
    "# - others → A\n",
    "merged_df[\"FinalGroup\"] = \"A\"\n",
    "merged_df.loc[has_name & (merged_df[\"isNameDup\"] == 1), \"FinalGroup\"] = merged_df[\"NameGroup\"]\n",
    "merged_df.loc[no_name & (merged_df[\"isIDDup_NoName\"] == 1), \"FinalGroup\"] = merged_df[\"IDOnlyGroup\"]\n",
    "\n",
    "# merged_df.loc[merged_df[\"IsDup\"] == 1, \"FinalGroup\"] = \"A\"\n",
    "\n",
    "cols_show = [\n",
    "    \"CleanedFileName\", \"SourceGroup\",\n",
    "    \"IsDup\",\n",
    "    \"Subject\", \"Subject_norm\",\n",
    "    \"ID\", \"ID_num\",\n",
    "    \"isNameDup\", \"NameGroup\",\n",
    "    \"isIDDup_Name\", \"NameIDGroup\",\n",
    "    \"isIDDup_NoName\", \"IDOnlyGroup\",\n",
    "    \"FinalGroup\",\n",
    "]\n",
    "# display(merged_df[cols_show].head(50))\n",
    "# display(merged_df[cols_show].tail(50))\n",
    "\n",
    "merged_df.to_excel(\"all_regroups.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8faa841",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = merged_df.rename(columns={\n",
    "    \"OutputPath\": \"Path\",\n",
    "    \"CleanedFileName\": \"Name\"\n",
    "})[[\"FullPath\", \"Path\", \"Name\", \"SourceGroup\", \"FinalGroup\"]]\n",
    "\n",
    "check_df.head(20)\n",
    "\n",
    "check_df[\"Name_length\"] = check_df[\"Name\"].str.len()\n",
    "\n",
    "check_df[\"Name\"] = check_df[\"Name\"].astype(str).str.slice(0, 48)\n",
    "\n",
    "check_df[\"Name\"] = check_df[\"Name\"].str.rstrip(\"-_\")\n",
    "\n",
    "check_df[\"Name_truncLength\"] = check_df[\"Name\"].str.len()\n",
    "\n",
    "check_df.to_excel(\"check_name_len.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae39e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: groups_output\\group1.xlsx_A.xlsx, len=3132)\n",
      "output: groups_output\\group1.xlsx_B.xlsx, len=246)\n",
      "output: groups_output\\group1.xlsx_C.xlsx, len=19)\n",
      "output: groups_output\\group1.xlsx_D.xlsx, len=3)\n",
      "output: groups_output\\group2.xlsx_A.xlsx, len=4601)\n",
      "output: groups_output\\group2.xlsx_B.xlsx, len=342)\n",
      "output: groups_output\\group2.xlsx_C.xlsx, len=20)\n",
      "output: groups_output\\group2.xlsx_D.xlsx, len=5)\n",
      "output: groups_output\\group2.xlsx_E.xlsx, len=3)\n",
      "output: groups_output\\group2.xlsx_F.xlsx, len=2)\n",
      "output: groups_output\\group3.xlsx_A.xlsx, len=3439)\n",
      "output: groups_output\\group3.xlsx_B.xlsx, len=123)\n",
      "output: groups_output\\group3.xlsx_C.xlsx, len=14)\n",
      "output: groups_output\\group3.xlsx_D.xlsx, len=4)\n",
      "output: groups_output\\group3.xlsx_E.xlsx, len=2)\n",
      "output: groups_output\\group4.xlsx_A.xlsx, len=2722)\n",
      "output: groups_output\\group4.xlsx_B.xlsx, len=229)\n",
      "output: groups_output\\group4.xlsx_C.xlsx, len=20)\n",
      "output: groups_output\\group4.xlsx_D.xlsx, len=5)\n",
      "output: groups_output\\group4.xlsx_E.xlsx, len=2)\n",
      "output: groups_output\\group4.xlsx_F.xlsx, len=2)\n"
     ]
    }
   ],
   "source": [
    "outdir = \"groups_output\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "export_df = check_df[[\"FullPath\", \"Path\", \"Name\", \"SourceGroup\", \"FinalGroup\"]].copy()\n",
    "export_df[\"Status\"] = 0\n",
    "\n",
    "for (sg, fg), subdf in export_df.groupby([\"SourceGroup\", \"FinalGroup\"]):\n",
    "    subdf_out = subdf[[\"FullPath\", \"Path\", \"Name\", \"Status\"]].copy()\n",
    "    \n",
    "    last_row = pd.DataFrame([[\"na\", \"na\", \"na\", 0]], columns=subdf_out.columns)\n",
    "    subdf_out = pd.concat([subdf_out, last_row], ignore_index=True)\n",
    "    \n",
    "    fname = f\"{sg}_{fg}.xlsx\"\n",
    "    fpath = os.path.join(outdir, fname)\n",
    "    \n",
    "    subdf_out.to_excel(fpath, index=False)\n",
    "    print(f\"output: {fpath}, len={len(subdf_out)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9ae720a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>FinalGroup</th>\n",
       "      <th>SourceGroup</th>\n",
       "      <th>Partitions</th>\n",
       "      <th>SSD</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>group1.xlsx</td>\n",
       "      <td>7&amp;9&amp;14&amp;16</td>\n",
       "      <td>TOSHIBA EXT</td>\n",
       "      <td>3131</td>\n",
       "      <td>245</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>group2.xlsx</td>\n",
       "      <td>2&amp;10&amp;11&amp;12&amp;13</td>\n",
       "      <td>SAMSUNG</td>\n",
       "      <td>4600</td>\n",
       "      <td>341</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>group3.xlsx</td>\n",
       "      <td>1&amp;3&amp;8&amp;15</td>\n",
       "      <td>KP Mirror</td>\n",
       "      <td>3438</td>\n",
       "      <td>122</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>group4.xlsx</td>\n",
       "      <td>4&amp;5&amp;6</td>\n",
       "      <td>SandmanFiles</td>\n",
       "      <td>2721</td>\n",
       "      <td>228</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "FinalGroup  SourceGroup     Partitions           SSD     A    B   C  D  E  F  \\\n",
       "0           group1.xlsx      7&9&14&16   TOSHIBA EXT  3131  245  18  2  0  0   \n",
       "1           group2.xlsx  2&10&11&12&13       SAMSUNG  4600  341  19  4  2  1   \n",
       "2           group3.xlsx       1&3&8&15     KP Mirror  3438  122  13  3  1  0   \n",
       "3           group4.xlsx          4&5&6  SandmanFiles  2721  228  19  4  1  1   \n",
       "\n",
       "FinalGroup  Total  \n",
       "0            3396  \n",
       "1            4967  \n",
       "2            3577  \n",
       "3            2974  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pivot_summary = (\n",
    "    check_df.pivot_table(\n",
    "        index=\"SourceGroup\",\n",
    "        columns=\"FinalGroup\",\n",
    "        values=\"Path\",\n",
    "        aggfunc=\"count\",\n",
    "        fill_value=0\n",
    "    )\n",
    ")\n",
    "\n",
    "pivot_summary[\"Total\"] = pivot_summary.sum(axis=1)\n",
    "\n",
    "pivot_summary = pivot_summary.reset_index()\n",
    "\n",
    "grp_info = {\n",
    "    \"group1.xlsx\": (\"7&9&14&16\",  \"TOSHIBA EXT\"),\n",
    "    \"group2.xlsx\": (\"2&10&11&12&13\", \"SAMSUNG\"),\n",
    "    \"group3.xlsx\": (\"1&3&8&15\",   \"KP Mirror\"),\n",
    "    \"group4.xlsx\": (\"4&5&6\",      \"SandmanFiles\"),\n",
    "}\n",
    "\n",
    "pivot_summary[\"Partitions\"] = pivot_summary[\"SourceGroup\"].map(lambda g: grp_info.get(g, (\"\", \"\"))[0])\n",
    "pivot_summary[\"SSD\"]        = pivot_summary[\"SourceGroup\"].map(lambda g: grp_info.get(g, (\"\", \"\"))[1])\n",
    "\n",
    "cols = list(pivot_summary.columns)\n",
    "new_order = [\"SourceGroup\", \"Partitions\", \"SSD\"] + [c for c in cols if c not in {\"SourceGroup\",\"Partitions\",\"SSD\"}]\n",
    "pivot_summary = pivot_summary[new_order]\n",
    "\n",
    "pivot_summary.to_excel(\"groups_pivot_summary_with_partitions.xlsx\", index=False)\n",
    "\n",
    "display(pivot_summary.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae84f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
